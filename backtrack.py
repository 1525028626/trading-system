import torch
import torch.nn as nn
import torch.optim as optim
import baostock as bs
import pandas as pd
import numpy as np
from tqdm import tqdm
import os
import random
import datetime
from dateutil.relativedelta import relativedelta

# å¼•å…¥å…¬å…±åº“
from stock_common import Config, StockTransformer, DataProvider, DataProcessor

def train_epoch_for_backtest(model, codes, end_date, learning_rate=0.0002, epochs=5):
    """
    é’ˆå¯¹ç‰¹å®šæˆªæ­¢æ—¥æœŸçš„å¾®è°ƒè®­ç»ƒå‡½æ•° (Incremental Learning)
    """
    print(f"[*] [Backtest] æ­£åœ¨é’ˆå¯¹æ—¥æœŸ {end_date} è¿›è¡Œå¾®è°ƒè®­ç»ƒ...")
    
    criterion = nn.HuberLoss()
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    
    all_X, all_y = [], []
    
    # ä¸ºäº†é€Ÿåº¦ï¼Œå›æº¯è®­ç»ƒæ—¶åªå–éƒ¨åˆ†è‚¡ç¥¨æˆ–å‡å°‘å›çœ‹å¤©æ•°
    # è¿™é‡Œæˆ‘ä»¬éšæœºå– 50 åªè‚¡ç¥¨è¿›è¡Œä»£è¡¨æ€§è®­ç»ƒï¼Œæˆ–è€…å…¨é‡è®­ç»ƒï¼ˆå–å†³äºè®¡ç®—èµ„æºï¼‰
    # å‡è®¾èµ„æºæœ‰é™ï¼Œæ¯æ¬¡éšæœºæŠ½ 50 åªæ ¸å¿ƒèµ„äº§
    sample_codes = random.sample(codes, min(len(codes), 50))
    
    valid_count = 0
    for code in sample_codes:
        # è·å–æˆªè‡³ end_date çš„æ•°æ®
        # fetch_days ä¸éœ€è¦å¤ªé•¿ï¼Œåªè¦è¦†ç›– LOOKBACK + ä¸€äº›è®­ç»ƒæ ·æœ¬å³å¯
        # æ¯”å¦‚ 200 å¤©ï¼Œå¤§çº¦æœ‰ 200 - 60 = 140 ä¸ªæ ·æœ¬
        df = DataProvider.fetch_stock_data(code, days=200, end_date=end_date)
        if df is None or len(df) < Config.LOOKBACK + 10: continue
        
        # === ä½¿ç”¨ DataProcessor ç»Ÿä¸€å¤„ç† ===
        data_values = DataProcessor.preprocess_data(df)
        if data_values is None: continue
        
        # æ„é€ æ»‘åŠ¨çª—å£æ ·æœ¬
        X_batch, y_batch = DataProcessor.create_sequences(data_values, Config.LOOKBACK)
        all_X.extend(X_batch)
        all_y.extend(y_batch)
            
        valid_count += 1

    if not all_X:
        print("[-] æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡æœ¬æ¬¡è®­ç»ƒ")
        return model

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% / 20%)
    split_idx = int(len(all_X) * 0.8)
    X_train_np = np.array(all_X[:split_idx])
    y_train_np = np.array(all_y[:split_idx])
    X_val_np = np.array(all_X[split_idx:])
    y_val_np = np.array(all_y[split_idx:])

    X_train = torch.tensor(X_train_np, dtype=torch.float32).to(Config.DEVICE)
    y_train = torch.tensor(y_train_np, dtype=torch.float32).view(-1, 1).to(Config.DEVICE)
    
    if len(X_val_np) > 0:
        X_val = torch.tensor(X_val_np, dtype=torch.float32).to(Config.DEVICE)
        y_val = torch.tensor(y_val_np, dtype=torch.float32).view(-1, 1).to(Config.DEVICE)
    else:
        X_val = None
    
    batch_size = 128
    model.train()
    
    for epoch in range(epochs):
        permutation = torch.randperm(X_train.size()[0])
        total_loss = 0
        total_acc = 0
        
        for i in range(0, X_train.size()[0], batch_size):
            indices = permutation[i:i+batch_size]
            optimizer.zero_grad()
            out = model(X_train[indices])
            
            # === Loss æ”¹è¿› ===
            base_loss = criterion(out, y_train[indices])
            close_idx = DataProcessor.FEATURE_COLS.index('close')
            last_close = X_train[indices, -1, close_idx].view(-1, 1)
            diff_pred = out - last_close
            diff_real = y_train[indices] - last_close
            penalty = torch.where(diff_pred * diff_real < 0, base_loss * 2.0, torch.zeros_like(base_loss))
            loss = base_loss + penalty.mean()
            
            # Acc
            acc = ((diff_pred * diff_real) > 0).float().mean().item()
            total_acc += acc
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item()
            
    # === éªŒè¯é›†è¯„ä¼° ===
    val_acc_str = "N/A"
    if X_val is not None:
        model.eval()
        with torch.no_grad():
            out_val = model(X_val)
            # Val Acc
            last_close_val = X_val[:, -1, close_idx].view(-1, 1)
            diff_pred_val = out_val - last_close_val
            diff_real_val = y_val - last_close_val
            val_acc = ((diff_pred_val * diff_real_val) > 0).float().mean().item()
            val_acc_str = f"{val_acc*100:.2f}%"
            
    print(f"    -> Train Loss: {total_loss/len(X_train)*batch_size:.6f} | Train Acc: {total_acc/len(X_train)*batch_size*100:.2f}% | Val Acc: {val_acc_str}")
    return model

def run_backtest_training():
    """
    å†å²å›æº¯è®­ç»ƒä¸»ç¨‹åº
    åŠŸèƒ½:
    1. è®¾å®šå›æº¯æ—¶é—´è½´ (Start Date -> End Date)
    2. é€æ­¥æ¨è¿›æ—¶é—´ï¼Œæ¨¡æ‹Ÿ"æ¯æœˆæ›´æ–°æ¨¡å‹"
    3. æ”¯æŒ"ä»å¤´è®­ç»ƒ"æˆ–"å¾®è°ƒç°æœ‰æ¨¡å‹"
    """
    print(f"\n{'='*50}\nâ³ å†å²å›æº¯è®­ç»ƒç³»ç»Ÿ (Backtrack Training)\n{'='*50}")
    
    # 1. è®¾ç½®å›æº¯èŒƒå›´
    start_date_str = input("è¯·è¾“å…¥å›æº¯å¼€å§‹æ—¥æœŸ (ä¾‹å¦‚ 2023-01-01): ").strip()
    months = int(input("è¯·è¾“å…¥å›æº¯æŒç»­æœˆæ•° (ä¾‹å¦‚ 12): ").strip())
    
    try:
        start_date = datetime.datetime.strptime(start_date_str, "%Y-%m-%d")
    except:
        print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD")
        return

    print("\nè¯·é€‰æ‹©æ¨¡å‹æ ¸å¿ƒ:")
    print("1. ğŸ›¡ï¸ ç¨³å¥æ¨¡å‹ (Conservative)")
    print("2. ğŸ”¥ æ¿€è¿›æ¨¡å‹ (Aggressive)")
    model_choice = input("è¯·è¾“å…¥æ•°å­— (1 æˆ– 2): ").strip()
    
    if model_choice == '2':
        model_type = 'aggressive'
        model_path = Config.MODEL_PATH_AGGRESSIVE
    else:
        model_type = 'conservative'
        model_path = Config.MODEL_PATH_CONSERVATIVE
        
    bs.login()
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = StockTransformer().to(Config.DEVICE)
    
    print("\nè¯·é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
    print("1. ğŸ£ ä»å¤´è®­ç»ƒ (From Scratch): å¿½ç•¥ç°æœ‰æ¨¡å‹ï¼Œä»å›æº¯å¼€å§‹æ—¥æœŸé‡æ–°è®­ç»ƒ")
    print("2. ğŸ§  å¾®è°ƒç°æœ‰æ¨¡å‹ (Fine-tune Existing): åŠ è½½ç°æœ‰æ¨¡å‹ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå›æº¯è®­ç»ƒ")
    mode_choice = input("è¯·è¾“å…¥æ•°å­— (1 æˆ– 2): ").strip()
    
    if mode_choice == '2' and os.path.exists(model_path):
        print(f"[*] åŠ è½½ç°æœ‰æ¨¡å‹ {model_path} ä½œä¸ºèµ·ç‚¹...")
        try:
            model.load_state_dict(torch.load(model_path, map_location=Config.DEVICE))
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            print("å°†è‡ªåŠ¨åˆ‡æ¢ä¸ºä»å¤´è®­ç»ƒæ¨¡å¼...")
    else:
        if mode_choice == '2':
            print(f"âš ï¸ æœªæ‰¾åˆ°ç°æœ‰æ¨¡å‹ {model_path}ï¼Œå°†åˆå§‹åŒ–æ–°æ¨¡å‹...")
        else:
            print("[*] åˆå§‹åŒ–æ–°æ¨¡å‹ä½œä¸ºèµ·ç‚¹...")

    # è·å–è‚¡ç¥¨åˆ—è¡¨ (åªè·å–ä¸€æ¬¡)
    codes = DataProvider.get_stock_list(mode=model_type)
    
    # 3. æ—¶é—´å¾ªç¯
    current_date = start_date
    for i in range(months):
        current_date_str = current_date.strftime("%Y-%m-%d")
        print(f"\n>>> [Step {i+1}/{months}] æ¨¡æ‹Ÿæ—¥æœŸ: {current_date_str}")
        
        # æ‰§è¡Œå¾®è°ƒè®­ç»ƒ
        # æ¨¡æ‹Ÿåœ¨è¿™ä¸ªæ—¥æœŸï¼Œæˆ‘ä»¬åªèƒ½çœ‹åˆ°è¿‡å»çš„æ•°æ®ï¼Œå¹¶åŸºäºæ­¤æ›´æ–°æ¨¡å‹
        model = train_epoch_for_backtest(model, codes, current_date_str)
        
        # ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ (å¯é€‰)
        # checkpoint_path = f"checkpoint_{current_date_str}.pth"
        # torch.save(model.state_dict(), checkpoint_path)
        
        # æ¨è¿›æ—¶é—´ (æ¯æœˆ)
        current_date = current_date + relativedelta(months=1)
        
    # 4. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\n{'='*50}")
    save_choice = input(f"å›æº¯è®­ç»ƒå®Œæˆã€‚æ˜¯å¦è¦†ç›–åŸæ¨¡å‹ {model_path}? (y/n): ")
    if save_choice.lower() == 'y':
        torch.save(model.state_dict(), model_path)
        print(f"âœ… æ¨¡å‹å·²æ›´æ–°å¹¶ä¿å­˜è‡³ {model_path}")
    else:
        backup_path = f"{model_path}.backtest_final.pth"
        torch.save(model.state_dict(), backup_path)
        print(f"âœ… æ¨¡å‹å·²å¦å­˜ä¸º {backup_path}")
        
    bs.logout()

if __name__ == "__main__":
    run_backtest_training()
