import torch
import torch.nn as nn
import torch.optim as optim
import baostock as bs
import pandas as pd
import numpy as np
from tqdm import tqdm
import os
import random

# å¼•å…¥å…¬å…±åº“
from stock_common import Config, StockTransformer, DataProvider, DataProcessor

from torch.utils.data import Dataset, DataLoader

# å®šä¹‰ worker åˆå§‹åŒ–å‡½æ•°ï¼Œç¡®ä¿å­è¿›ç¨‹èƒ½ç™»å½• baostock
def worker_init_fn(worker_id):
    bs.login() 

class StockDataset(Dataset):
    def __init__(self, codes, train_mode, fetch_days, lookback):
        self.codes = codes
        self.train_mode = train_mode
        self.fetch_days = fetch_days
        self.lookback = lookback
    
    def __len__(self):
        return len(self.codes)
    
    def __getitem__(self, idx):
        code = self.codes[idx]
        
        # 1. è·å–æ•°æ® (å¢é‡æ›´æ–°é€»è¾‘åœ¨ fetch_stock_data å†…éƒ¨å¤„ç†)
        # æ³¨æ„: è¿™é‡Œè°ƒç”¨çš„æ˜¯ stock_common.py é‡Œçš„ fetch_stock_data
        df = DataProvider.fetch_stock_data(code, days=self.fetch_days)
        if df is None or len(df) < self.lookback + 5:
            return [], []
            
        # 2. å¢é‡æ¨¡å¼ä¸‹çš„ç‰¹æ®Šè¿‡æ»¤ (åƒµå°¸è‚¡/åœç‰Œ)
        if self.train_mode == '2':
            # åœç‰Œæ£€æŸ¥
            try:
                last_dt = pd.to_datetime(str(df.iloc[-1]['date']))
                if (pd.Timestamp.now() - last_dt).days > 5:
                    return [], []
            except: pass
            
            # æˆäº¤é‡æ£€æŸ¥
            try:
                recent_vol = pd.to_numeric(df['volume'].tail(5)).mean()
                if recent_vol < 10000: # æ—¥å‡æˆäº¤ä¸è¶³
                    return [], []
            except: pass
        
        # 3. é¢„å¤„ç†
        data_values = DataProcessor.preprocess_data(df)
        if data_values is None: return [], []
        
        # 4. ç”Ÿæˆåºåˆ—
        X_batch, y_batch = DataProcessor.create_sequences(data_values, self.lookback)
        
        # ç®€å• NaN æ£€æŸ¥
        if np.isnan(X_batch).any() or np.isnan(y_batch).any():
            return [], []
            
        return X_batch, y_batch

# å°†å¤šä¸ª batch (æ¯ä¸ªæ˜¯ [X_list, y_list]) åˆå¹¶ä¸ºä¸€ä¸ªå¤§åˆ—è¡¨
def collate_fn(batch):
    X_all = []
    y_all = []
    for X, y in batch:
        if X: # X ä¸ä¸ºç©ºåˆ—è¡¨
            X_all.extend(X)
            y_all.extend(y)
    return X_all, y_all

JOURNAL_FILE = "ai_trading_journal.csv"

def get_feedback_data(model_type='conservative'):
    """
    è¯»å–é”™é¢˜æœ¬ï¼Œæ„å»ºé’ˆå¯¹æ€§è®­ç»ƒæ•°æ® (Hard Example Mining)
    æ ¹æ®å½“å‰è®­ç»ƒçš„æ¨¡å‹ç±»å‹ï¼Œåªæå–å¯¹åº”ç±»å‹çš„é”™é¢˜
    
    Args:
        model_type: 'conservative' æˆ– 'aggressive'
        
    Returns:
        fb_X: é”™é¢˜æ ·æœ¬çš„ç‰¹å¾åºåˆ—åˆ—è¡¨
        fb_y: é”™é¢˜æ ·æœ¬çš„çœŸå®ç›®æ ‡å€¼åˆ—è¡¨
    """
    if not os.path.exists(JOURNAL_FILE):
        return [], []

    df = pd.read_csv(JOURNAL_FILE)
    
    # å…¼å®¹æ—§ç‰ˆ CSV (æ²¡æœ‰ model_type åˆ—çš„æƒ…å†µ)
    if 'model_type' not in df.columns:
        print("âš ï¸ é”™é¢˜æœ¬æœªåŒ…å«æ¨¡å‹ç±»å‹ä¿¡æ¯ï¼Œå°†è·³è¿‡ç­›é€‰ï¼ˆå¯èƒ½å¯¼è‡´æ¨¡å‹æ··æ·†ï¼‰")
        # ç­›é€‰å‡ºå·²ç»éªŒè¯è¿‡ä¸”è¯¯å·®è¾ƒå¤§çš„è®°å½• (Error > 3.0%)
        mistakes = df[(df['status'] == 'verified') & (df['error'] > 3.0)]
    else:
        # ç­›é€‰: verified + error > 3.0% + model_type åŒ¹é…
        mistakes = df[
            (df['status'] == 'verified') & 
            (df['error'] > 3.0) & 
            (df['model_type'] == model_type)
        ]
    
    if mistakes.empty: return [], []

    print(f"[*] é”™é¢˜æœ¬åŠ è½½ ({model_type}): å‘ç° {len(mistakes)} ä¸ªä¸¥é‡é”™è¯¯ï¼Œå‡†å¤‡æå–æ•°æ®...")
    
    fb_X, fb_y = [], []
    for _, row in tqdm(mistakes.iterrows(), total=len(mistakes)):
        code = row['code']
        error_date = row['date']
        
        # è·å–è¶³å¤Ÿé•¿çš„æ•°æ®ä»¥æ„å»ºåºåˆ—
        df = DataProvider.fetch_stock_data(code, days=Config.LOOKBACK + 100)
        if df is None: continue
        
        # æ‰¾åˆ°é”™é¢˜å‘ç”Ÿçš„æ—¥æœŸ
        target_rows = df[df['date'] == error_date]
        if target_rows.empty:
            print(f"[-] {code}: æœªæ‰¾åˆ°æ—¥æœŸ {error_date} (æ•°æ®èŒƒå›´: {df['date'].iloc[0]} ~ {df['date'].iloc[-1]})")
            continue
        
        # æ ¸å¿ƒä¿®å¤: è·å–è¯¥è¡Œåœ¨ df ä¸­çš„ä½ç½® (0-based integer position)ï¼Œè€Œä¸æ˜¯ Label Index
        target_label_idx = target_rows.index[0]
        
        # === ä½¿ç”¨ DataProcessor ç»Ÿä¸€å¤„ç† ===
        # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½ä¼šå› ä¸ºæ¸…æ´— Inf è€Œåˆ é™¤è¡Œï¼Œå¯¼è‡´é•¿åº¦å˜åŒ–
        data_values = DataProcessor.preprocess_data(df)
        if data_values is None: 
            print(f"[-] {code}: é¢„å¤„ç†å¤±è´¥ (å¯èƒ½æ•°æ®å…¨ç©º)")
            continue
        
        # é‡æ–°å®šä½ target_pos (åœ¨ data_values ä¸­çš„ä½ç½®)
        # æˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿ preprocess_data çš„æ¸…æ´—è¿‡ç¨‹æ¥æ‰¾åˆ°å¯¹åº”å…³ç³»
        # æˆ–è€…æ›´ç®€å•ï¼šDataProcessor.preprocess_data å…¶å®æ˜¯åŸºäº FEATURE_COLS æ¸…æ´—çš„
        
        # 1. å¤ç°æ¸…æ´—é€»è¾‘æ‰¾åˆ°ä¿ç•™ä¸‹æ¥çš„ Index
        temp_df = df[DataProcessor.FEATURE_COLS].copy()
        temp_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        valid_indices = temp_df.dropna().index
        
        # 2. æ£€æŸ¥æˆ‘ä»¬çš„ç›®æ ‡è¡Œæ˜¯å¦è¿˜åœ¨
        if target_label_idx not in valid_indices:
            print(f"[-] {code}: ç›®æ ‡è¡Œ {error_date} å› åŒ…å« Inf/NaN è¢«æ¸…æ´—ï¼Œè·³è¿‡")
            continue
            
        # 3. è·å–æ–°çš„ä½ç½® (0-based index in data_values)
        try:
            target_pos = valid_indices.get_loc(target_label_idx)
        except:
            # å…¼å®¹æ€§å†™æ³•
            target_pos = list(valid_indices).index(target_label_idx)
            
        if target_pos < Config.LOOKBACK: 
            print(f"[-] {code}: å†å²æ•°æ®ä¸è¶³ ({target_pos} < {Config.LOOKBACK})")
            continue

        # æ³¨æ„: preprocess_data å·²ç»åšäº† Log å˜æ¢ï¼Œç°åœ¨éœ€è¦ Scale å¹¶æ„å»ºåºåˆ—
        # æˆ‘ä»¬åªå…³å¿ƒ target_pos è¿™ä¸ªç‚¹çš„é¢„æµ‹
        # create_sequences ä¼šè¿”å›æ‰€æœ‰å¯èƒ½çš„åºåˆ—ï¼Œæˆ‘ä»¬åªå– target_pos å¯¹åº”çš„é‚£ä¸ª
        
        # æš‚æ—¶æ‰‹åŠ¨å¤„ç†ä»¥ç²¾ç¡®å®šä½ target_pos
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data_values)
        
        # åºåˆ—: [target_pos - LOOKBACK : target_pos]
        seq_x = data_scaled[target_pos - Config.LOOKBACK : target_pos]
        seq_y = data_scaled[target_pos, DataProcessor.FEATURE_COLS.index('close')]
        
        fb_X.append(seq_x)
        fb_y.append(seq_y)
        
    return fb_X, fb_y

def train_model(model_type=None, mode=None):
    """
    ä¸»è®­ç»ƒæµç¨‹
    åŒ…å«:
    1. æ¨¡å¼é€‰æ‹© (ç¨³å¥/æ¿€è¿›)
    2. è®­ç»ƒç±»å‹é€‰æ‹© (å…¨é‡/å¢é‡)
    3. Phase 1: å¤§è§„æ¨¡åŸºç¡€è®­ç»ƒ
    4. Phase 2: é”™é¢˜æœ¬ç²¾è°ƒ
    
    Args:
        model_type: 'conservative' æˆ– 'aggressive' (å¦‚æœä¸º None åˆ™äº¤äº’å¼é€‰æ‹©)
        mode: '1' (å…¨é‡) æˆ– '2' (å¢é‡) (å¦‚æœä¸º None åˆ™äº¤äº’å¼é€‰æ‹©)
    """
    print(f"\n{'='*50}\nğŸš€ AI è®­ç»ƒæ§åˆ¶å° (Training Console)\n{'='*50}")
    
    # 1. é€‰æ‹©æ¨¡å‹ç±»å‹
    if model_type is None:
        print("è¯·é€‰æ‹©æ¨¡å‹ç±»å‹:")
        print("1. ğŸ›¡ï¸ ç¨³å¥æ¨¡å‹ (Conservative): ä»…è®­ç»ƒ HS300+ZZ500 (æ ¸å¿ƒèµ„äº§)")
        print("2. ğŸ”¥ æ¿€è¿›æ¨¡å‹ (Aggressive): è®­ç»ƒå…¨å¸‚åœºè‚¡ç¥¨ (åŒ…å«é¢˜æ/å°ç¥¨)")
        model_type_input = input("\nè¯·è¾“å…¥æ•°å­— (1 æˆ– 2): ").strip()
        model_type = 'aggressive' if model_type_input == '2' else 'conservative'
    
    if model_type == 'aggressive':
        model_path = Config.MODEL_PATH_AGGRESSIVE
        print(f"\n[!] å·²é€‰æ‹©: ğŸ”¥ æ¿€è¿›æ¨¡å‹ (ä¿å­˜è·¯å¾„: {model_path})")
    else:
        model_path = Config.MODEL_PATH_CONSERVATIVE
        print(f"\n[!] å·²é€‰æ‹©: ğŸ›¡ï¸ ç¨³å¥æ¨¡å‹ (ä¿å­˜è·¯å¾„: {model_path})")

    # 2. é€‰æ‹©è®­ç»ƒæ¨¡å¼
    if mode is None:
        print("\nè¯·é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
        print("1. ğŸ†• å…¨é‡è®­ç»ƒ (Full Train): åˆ é™¤æ—§æ¨¡å‹ï¼Œä»é›¶å¼€å§‹å­¦ä¹  (é€‚åˆå‘¨æœ«/å¤§è§„æ¨¡æ›´æ–°)")
        print("2. ğŸ§  å¢é‡ç²¾è°ƒ (Incremental): åŠ è½½æ—§æ¨¡å‹ï¼Œåªå­¦æ–°æ•°æ®å’Œé”™é¢˜ (é€‚åˆæ¯æ—¥æ”¶ç›˜å)")
        mode = input("\nè¯·è¾“å…¥æ•°å­— (1 æˆ– 2): ").strip()
    
    # === é…ç½®è®­ç»ƒå‚æ•° ===
    if mode == '1':
        print("\n[!] å·²é€‰æ‹©: å…¨é‡è®­ç»ƒæ¨¡å¼")
        if os.path.exists(model_path):
            print(f"[*] åˆ é™¤æ—§æ¨¡å‹ {model_path} ...")
            try: os.remove(model_path)
            except: pass
        
        # å…¨é‡è®­ç»ƒå‚æ•°ï¼šé«˜å­¦ä¹ ç‡ï¼Œå¤šè½®æ¬¡
        LEARNING_RATE = 0.001 
        EPOCHS = Config.EPOCHS # é»˜è®¤ 40
        load_existing = False
        
    elif mode == '2':
        print("\n[!] å·²é€‰æ‹©: å¢é‡ç²¾è°ƒæ¨¡å¼")
        if not os.path.exists(model_path):
            print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ç°æœ‰æ¨¡å‹ {model_path}ï¼Œæ— æ³•è¿›è¡Œå¢é‡è®­ç»ƒï¼è¯·å…ˆé€‰æ‹©å…¨é‡è®­ç»ƒã€‚")
            return
            
        # å¢é‡è®­ç»ƒå‚æ•°ï¼šä½å­¦ä¹ ç‡ï¼Œå°‘è½®æ¬¡ (é˜²æ­¢é—å¿˜)
        LEARNING_RATE = 0.0002 
        EPOCHS = 10 
        load_existing = True
        
    else:
        print("æ— æ•ˆè¾“å…¥ï¼Œé€€å‡ºã€‚")
        return

    bs.login()
    
    # åˆå§‹åŒ–æ¨¡å‹
    # æ³¨æ„ï¼šå› ä¸º INPUT_DIM å¯èƒ½å˜åŒ–ï¼Œå¦‚æœåŠ è½½æ—§æ¨¡å‹å½¢çŠ¶ä¸åŒ¹é…ä¼šæŠ¥é”™
    model = StockTransformer().to(Config.DEVICE)
    
    if load_existing:
        print(f"[*] æ­£åœ¨åŠ è½½ç°æœ‰æ¨¡å‹æƒé‡: {model_path}...")
        try:
            state_dict = torch.load(model_path, map_location=Config.DEVICE)
            
            # æ£€æŸ¥ input_net.0.weight çš„å½¢çŠ¶æ˜¯å¦åŒ¹é…å½“å‰ Config.INPUT_DIM
            if state_dict['input_net.0.weight'].shape[1] != Config.INPUT_DIM:
                print(f"âš ï¸ æ¨¡å‹è¾“å…¥ç»´åº¦ä¸åŒ¹é… (æ—§: {state_dict['input_net.0.weight'].shape[1]}, æ–°: {Config.INPUT_DIM})")
                print("âŒ æ— æ³•å¢é‡è®­ç»ƒï¼Œè¯·é€‰æ‹© [1. å…¨é‡è®­ç»ƒ] é‡å»ºæ¨¡å‹ï¼")
                return
                
            model.load_state_dict(state_dict)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            print("âŒ è¯·é‡æ–°é€‰æ‹© [1. å…¨é‡è®­ç»ƒ] ä»¥é€‚é…æ–°ç‰¹å¾ï¼")
            return
    else:
        print("[*] åˆå§‹åŒ–å…¨æ–° Transformer æ¨¡å‹...")

    criterion = nn.HuberLoss() 
    
    # ==========================
    # Phase 1: åŸºç¡€è®­ç»ƒ (å¤ä¹ /é‡ä¿®)
    # ==========================
    print(f"\n>>> é˜¶æ®µä¸€ï¼šå…¨å¸‚åœºæ‰«æè®­ç»ƒ (LR={LEARNING_RATE}, Epochs={EPOCHS})")
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨ (æ ¹æ®æ¨¡å‹ç±»å‹)
    codes = DataProvider.get_stock_list(mode=model_type)
    
    all_X, all_y = [], []
    
    # å¦‚æœæ˜¯å¢é‡è®­ç»ƒï¼ŒåªéšæœºæŠ½æŸ¥ 30% çš„è‚¡ç¥¨è¿›è¡Œâ€œå¤ä¹ â€ï¼ŒèŠ‚çœæ—¶é—´
    # å¦‚æœæ˜¯å…¨é‡è®­ç»ƒï¼Œä½¿ç”¨æ‰€æœ‰è‚¡ç¥¨
    if mode == '2':
        # æ¿€è¿›æ¨¡å¼ä¸‹ï¼Œè‚¡ç¥¨æ± å¤ªå¤§ (5000+)ï¼Œä¼˜åŒ–é‡‡æ ·ç­–ç•¥:
        # 1. åŸºç¡€é‡‡æ ·ç‡é™è‡³ 10% (0.1)ï¼Œä¿è¯å¹¿åº¦ä½†å‡å°‘æ•°é‡
        # 2. ç¡¬ä¸Šé™ (Max Cap) é™åˆ¶ä¸º 500 åªï¼Œé˜²æ­¢æ—¶é—´è¿‡é•¿
        # 3. åŠ¨æ€è¿‡æ»¤: åœ¨ä¸‹è½½æ•°æ®åï¼Œå¦‚æœå‘ç°æˆäº¤é‡è¿‡ä½(åƒµå°¸è‚¡)æˆ–åœç‰Œï¼Œç›´æ¥ä¸¢å¼ƒ
        
        max_samples = 500
        sample_ratio = 0.1
        
        # è®¡ç®—é‡‡æ ·æ•°é‡
        target_size = min(int(len(codes) * sample_ratio), max_samples)
        
        print(f"[*] å¢é‡æ¨¡å¼ï¼šéšæœºæŠ½å– {target_size} åªè‚¡ç¥¨è¿›è¡Œå¤ä¹  (Pool: {len(codes)})...")
        training_codes = random.sample(codes, target_size)
    else:
        print(f"[*] å…¨é‡æ¨¡å¼ï¼šä½¿ç”¨å…¨éƒ¨ {len(codes)} åªè‚¡ç¥¨è¿›è¡Œè®­ç»ƒ...")
        training_codes = codes

    # ä½¿ç”¨ DataLoader å¹¶è¡ŒåŠ è½½æ•°æ®
    # å¢é‡æ¨¡å¼åªçœ‹æœ€è¿‘ 300 å¤©ï¼Œå…¨é‡æ¨¡å¼çœ‹æœ€è¿‘ 1000 å¤©
    fetch_days = 300 if mode == '2' else Config.LOOKBACK + 500
    
    print(f"[*] å¯åŠ¨å¹¶è¡Œæ•°æ®åŠ è½½ (ä½¿ç”¨ 4 ä¸ª worker)...")
    dataset = StockDataset(training_codes, train_mode=mode, fetch_days=fetch_days, lookback=Config.LOOKBACK)
    
    # Windows ä¸‹å»ºè®® num_workers è®¾ä¸º 0 ä»¥ä¿è¯ç¨³å®šæ€§ (é¿å… WinError 10053 å’Œå†…å­˜æº¢å‡º)
    # è™½ç„¶æ˜¯å•è¿›ç¨‹ï¼Œä½†å› ä¸ºæœ‰å¢é‡ç¼“å­˜ï¼Œé€Ÿåº¦ä¾ç„¶å¾ˆå¿«
    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=collate_fn)
    
    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    
    print(f"\n>>> å¼€å§‹æµå¼è®­ç»ƒ (Stream Training)...")
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        total_acc = 0
        total_samples = 0
        
        # è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰ Epoch è¿›åº¦
        pbar = tqdm(dataloader, total=len(training_codes), desc=f"Epoch {epoch+1}/{EPOCHS}")
        
        for X_batch_list, y_batch_list in pbar:
            # X_batch_list æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†ä¸€åªè‚¡ç¥¨çš„æ‰€æœ‰åˆ‡ç‰‡æ ·æœ¬
            # ä¾‹å¦‚: Tensor shape [200, 60, 30]
            
            if len(X_batch_list) == 0: continue
            
            # å°†åˆ—è¡¨è½¬ä¸º Tensor
            # æ³¨æ„ï¼šè¿™é‡ŒåªåŠ è½½ä¸€åªè‚¡ç¥¨çš„æ•°æ®åˆ° GPUï¼Œå†…å­˜å ç”¨æå°
            X_stock = torch.tensor(np.array(X_batch_list), dtype=torch.float32).to(Config.DEVICE)
            y_stock = torch.tensor(np.array(y_batch_list), dtype=torch.float32).view(-1, 1).to(Config.DEVICE)
            
            # åœ¨è‚¡ç¥¨å†…éƒ¨è¿›è¡Œå°æ‰¹é‡è®­ç»ƒ (Mini-batch within Stock)
            # æˆ–è€…ç›´æ¥æŠŠæ•´åªè‚¡ç¥¨ä½œä¸ºä¸€ä¸ª Batch è®­ç»ƒ (å¦‚æœæ ·æœ¬æ•°ä¸å¤šï¼Œæ¯”å¦‚ 200 ä¸ªï¼Œå®Œå…¨å¯ä»¥)
            # ä¸ºäº†ç¨³å®šï¼Œæˆ‘ä»¬ç›´æ¥æ•´åªè‚¡ç¥¨è®­ç»ƒ
            
            optimizer.zero_grad()
            out = model(X_stock)
            
            # === Loss è®¡ç®— ===
            base_loss = criterion(out, y_stock)
            
            # è·å–è¾“å…¥åºåˆ—çš„æœ€åä¸€å¤©æ”¶ç›˜ä»· (ä½œä¸ºåŸºå‡†)
            close_idx = DataProcessor.FEATURE_COLS.index('close')
            last_close = X_stock[:, -1, close_idx].view(-1, 1)
            
            diff_pred = out - last_close
            diff_real = y_stock - last_close
            
            penalty = torch.where(diff_pred * diff_real < 0, base_loss * 2.0, torch.zeros_like(base_loss))
            loss = base_loss + penalty.mean()
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            # ç»Ÿè®¡
            current_loss = loss.item() * len(X_stock)
            total_loss += current_loss
            
            acc = ((diff_pred * diff_real) > 0).float().sum().item()
            total_acc += acc
            total_samples += len(X_stock)
            
            # æ›´æ–°è¿›åº¦æ¡åç¼€
            pbar.set_postfix({'Loss': f"{total_loss/total_samples:.6f}", 'Acc': f"{total_acc/total_samples*100:.2f}%"})

        # Epoch ç»“æŸæ‰“å°
        avg_loss = total_loss / total_samples if total_samples > 0 else 0
        avg_acc = total_acc / total_samples * 100 if total_samples > 0 else 0
        print(f"Phase 1 | Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.6f} | Avg Acc: {avg_acc:.2f}%")

    # ==========================
    # Phase 2: é”™é¢˜æœ¬ç²¾è°ƒ (Feedback Loop)
    # ==========================
    print("\n>>> é˜¶æ®µäºŒï¼šé”™é¢˜æœ¬ç²¾è°ƒ (Hard Example Mining)")
    bs.logout()  # å…ˆæ–­å¼€æ—§è¿æ¥
    import time
    time.sleep(1) # ä¼‘æ¯ä¸€ç§’
    bs.login()   # é‡æ–°å»ºç«‹æ–°è¿æ¥
    fb_X, fb_y = get_feedback_data(model_type=model_type)
    
    if len(fb_X) > 0:
        X_fb = torch.tensor(np.array(fb_X), dtype=torch.float32).to(Config.DEVICE)
        y_fb = torch.tensor(np.array(fb_y), dtype=torch.float32).view(-1, 1).to(Config.DEVICE)
        
        print(f"[*] é’ˆå¯¹ {len(X_fb)} ä¸ªä¸¥é‡é”™è¯¯æ ·æœ¬è¿›è¡Œç‰¹è®­...")
        
        # ç‰¹è®­ä½¿ç”¨æä½çš„å­¦ä¹ ç‡ï¼Œé˜²æ­¢ç ´åå·²æœ‰çŸ¥è¯†
        # å¢é‡æ¨¡å¼ä¸‹ï¼Œå¯¹é”™é¢˜æ›´æ•æ„Ÿ
        ft_lr = 0.0001 if mode == '1' else 0.0002
        optimizer_ft = optim.SGD(model.parameters(), lr=ft_lr, momentum=0.9)
        
        ft_epochs = 20 # æ— è®ºå“ªç§æ¨¡å¼ï¼Œé”™é¢˜éƒ½è¦çœ‹ 20 é
        
        for epoch in range(ft_epochs):
            model.train()
            optimizer_ft.zero_grad()
            out = model(X_fb)
            loss = criterion(out, y_fb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
            optimizer_ft.step()
            
            if (epoch+1) % 5 == 0:
                print(f"Fine-tune Epoch {epoch+1} | Loss: {loss.item():.6f}")
        print("âœ… é”™é¢˜å¸æ”¶å®Œæ¯•ï¼")
    else:
        print("[*] æš‚æ— ä¸¥é‡é”™é¢˜æ•°æ®ï¼Œè·³è¿‡å¾®è°ƒã€‚")

    torch.save(model.state_dict(), model_path)
    print(f"\n{'='*50}")
    print(f"âœ… {('å…¨é‡' if mode=='1' else 'å¢é‡')}è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    print(f"{'='*50}\n")
    # bs.logout() # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œ logoutï¼Œå¦åˆ™ä¼šæ–­å¼€ auto_run çš„è¿æ¥

if __name__ == "__main__":
    try:
        train_model()
    finally:
        bs.logout()